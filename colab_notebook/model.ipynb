{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9c2be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Open In Colab\n",
    "\n",
    "# Cell 1: Mount Google Drive & Install Libraries\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install rasterio\n",
    "!pip install scikit-image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Cell 2: Load Data, Pad, and Create Tiles\n",
    "# Define your file paths\n",
    "input_path = '/content/drive/MyDrive/deforestation_data/deforestation_input_image.tif'\n",
    "labels_path = '/content/drive/MyDrive/deforestation_data/deforestation_labels.tif'\n",
    "\n",
    "# Load the full input image and labels\n",
    "with rasterio.open(input_path) as src:\n",
    "    full_input_image = src.read()\n",
    "\n",
    "with rasterio.open(labels_path) as src:\n",
    "    full_labels = src.read(1)\n",
    "\n",
    "print(\"Original input image shape:\", full_input_image.shape)\n",
    "print(\"Original labels shape:\", full_labels.shape)\n",
    "\n",
    "# Normalize the input image data\n",
    "full_input_image = full_input_image.astype('float32') / 3000.0\n",
    "\n",
    "# Pad the images to be a multiple of 256\n",
    "tile_size = 256\n",
    "padded_height = (full_input_image.shape[1] // tile_size + 1) * tile_size\n",
    "padded_width = (full_input_image.shape[2] // tile_size + 1) * tile_size\n",
    "\n",
    "# Use numpy.pad for padding\n",
    "input_padded = np.pad(full_input_image, ((0, 0), (0, padded_height - full_input_image.shape[1]), (0, padded_width - full_input_image.shape[2])), mode='constant')\n",
    "labels_padded = np.pad(full_labels, ((0, padded_height - full_labels.shape[0]), (0, padded_width - full_labels.shape[1])), mode='constant')\n",
    "\n",
    "# Transpose the input image to (height, width, channels) for slicing\n",
    "input_padded = np.transpose(input_padded, (1, 2, 0))\n",
    "\n",
    "# Create tiles\n",
    "input_tiles = []\n",
    "label_tiles = []\n",
    "for y in range(0, padded_height, tile_size):\n",
    "    for x in range(0, padded_width, tile_size):\n",
    "        input_tile = input_padded[y:y + tile_size, x:x + tile_size, :]\n",
    "        label_tile = labels_padded[y:y + tile_size, x:x + tile_size]\n",
    "        input_tiles.append(input_tile)\n",
    "        label_tiles.append(np.expand_dims(label_tile, axis=-1))\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(input_tiles)\n",
    "y = np.array(label_tiles)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nNumber of training tiles:\", len(X_train))\n",
    "print(\"Number of validation tiles:\", len(X_val))\n",
    "print(\"Shape of a single tile:\", X_train[0].shape)\n",
    "print(\"Shape of a single label tile:\", y_train[0].shape)\n",
    "\n",
    "\n",
    "# Cell 3: Build and Train the UNet Model\n",
    "# UNet Model Definition\n",
    "def unet_model(input_shape=(256, 256, 6)):\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
    "\n",
    "    # Decoder\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    up4 = Conv2D(64, 2, activation='relu', padding='same')(up4)\n",
    "    merge4 = concatenate([conv2, up4], axis=3)\n",
    "    conv4 = Conv2D(64, 3, activation='relu', padding='same')(merge4)\n",
    "    conv4 = Conv2D(64, 3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    up5 = Conv2D(32, 2, activation='relu', padding='same')(up5)\n",
    "    merge5 = concatenate([conv1, up5], axis=3)\n",
    "    conv5 = Conv2D(32, 3, activation='relu', padding='same')(merge5)\n",
    "    conv5 = Conv2D(32, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    # Output layer\n",
    "    output = Conv2D(1, 1, activation='sigmoid')(conv5)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model = unet_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
